# -*- coding: utf-8 -*-
"""kapsel andat fix

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YZ77AZToFkXJDuMOFFE46ZCSZgLpRVFP

## Library
"""

!pip install gensim

!pip install gensim scikit-learn

!pip install xgboost

!pip install tensorflow

import pandas as pd
import re
import numpy as np
from sklearn.model_selection import train_test_split
from gensim.parsing.preprocessing import preprocess_string
from gensim.models import Word2Vec
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score
from collections import Counter
from sklearn.svm import LinearSVC
from catboost import CatBoostClassifier
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, BatchNormalization, Dense, InputLayer, GlobalMaxPooling1D
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
import matplotlib.pyplot as plt
import nltk
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

"""#Data Preparation"""

import pandas as pd
df = pd.read_csv(
    "/content/mental health dataset.csv",
    encoding="latin-1",
    engine="python",
    quoting=0,
    sep=',',
    on_bad_lines="skip"
)
df

"""#Data Cleaning"""

# hapus baris kosong
df = df.dropna(subset=["statement", "status"])

# hapus duplikat
df = df.drop_duplicates()

# Satukan label sesuai kebutuhanmu
df['status'] = df['status'].replace({
    'Bipolar': 'Personality disorder',
    'Stress': 'Anxiety'
})

print(df.head())
print(df['status'].value_counts())

def normalisasi(teks):
    teks = teks.lower()

    # keep a-z, 0-9, and basic punctuation only
    teks = re.sub(r"[^a-z0-9.,!?()' ]+", " ", teks)

    # contractions
    teks = re.sub(r"\s*:\s*", ":", teks)
    teks = re.sub(r"\'s", " 's", teks)
    teks = re.sub(r"\'ve", " 've", teks)
    teks = re.sub(r"n\'t", " n't", teks)
    teks = re.sub(r"\'re", " 're", teks)
    teks = re.sub(r"\'d", " 'd", teks)
    teks = re.sub(r"\'ll", " 'll", teks)

    # space around punctuation
    teks = re.sub(r",", " , ", teks)
    teks = re.sub(r"!", " ! ", teks)
    teks = re.sub(r"\(", " ( ", teks)
    teks = re.sub(r"\)", " ) ", teks)
    teks = re.sub(r"\?", " ? ", teks)

    # remove multi spaces
    teks = re.sub(r"\s+", " ", teks)

    return teks.strip()

df['status'].unique()

"""#Preprocessing

## Data spliting
"""

valid_classes = df['status'].value_counts()[lambda x: x >= 2].index
df = df[df['status'].isin(valid_classes)]

# === 3. SPLIT DATA ===
train_val_df, test_df = train_test_split(df, test_size=0.2, stratify=df['status'], random_state=42)
train_opt_df, val_df = train_test_split(train_val_df, test_size=0.2, stratify=train_val_df['status'], random_state=42)

"""##Tokenisasi"""

lemmatizer = WordNetLemmatizer()

def preprocess_tokens(text):
    tokens = preprocess_string(text)
    return [lemmatizer.lemmatize(token) for token in tokens]

train_tokens = train_opt_df['statement'].apply(preprocess_tokens)
val_tokens = val_df['statement'].apply(preprocess_tokens)
test_tokens = test_df['statement'].apply(preprocess_tokens)

# Pilih kata-kata paling sering muncul (vocabulary relevan)
n = 5000
all_tokens = [token for tokens in train_tokens for token in tokens]
token_counts = Counter(all_tokens)
vocab = set([token for token, _ in token_counts.most_common(n)])

def filter_vocab(tokens):
    return [token for token in tokens if token in vocab]

train_filtered = train_tokens.apply(filter_vocab)
val_filtered = val_tokens.apply(filter_vocab)
test_filtered = test_tokens.apply(filter_vocab)

"""##Word Embedding"""

embedding_size = 128
w2v_model = Word2Vec(sentences=train_filtered.tolist(), vector_size=embedding_size, window=5, min_count=1, workers=4)

def encode_w2v(tokens, model, max_len=30):
    vecs = []
    for token in tokens[:max_len]:
        if token in model.wv:
            vecs.append(model.wv[token])
        else:
            vecs.append(np.zeros(embedding_size))
    while len(vecs) < max_len:
        vecs.append(np.zeros(embedding_size))
    return np.array(vecs)

X_train_w2v = np.stack(train_filtered.apply(lambda x: encode_w2v(x, w2v_model)))
X_val_w2v = np.stack(val_filtered.apply(lambda x: encode_w2v(x, w2v_model)))
X_test_w2v = np.stack(test_filtered.apply(lambda x: encode_w2v(x, w2v_model)))

"""##Label Encoding"""

label_map = {'Anxiety': 0, 'Depression': 1, 'Normal': 2, 'Personality disorder': 3, 'Suicidal': 4}
y_train = train_opt_df['status'].map(label_map).values
y_val = val_df['status'].map(label_map).values
y_test = test_df['status'].map(label_map).values

# Mengecek jumlah data latih (X_train_w2v dan y_train)
print(f"Jumlah data latih: {X_train_w2v.shape[0]} samples")
print(f"Jumlah label latih: {y_train.shape[0]} samples")

# Mengecek jumlah data validasi (X_val_w2v dan y_val)
print(f"Jumlah data validasi: {X_val_w2v.shape[0]} samples")
print(f"Jumlah label validasi: {y_val.shape[0]} samples")

# Mengecek jumlah data uji (X_test_w2v dan y_test)
print(f"Jumlah data uji: {X_test_w2v.shape[0]} samples")
print(f"Jumlah label uji: {y_test.shape[0]} samples")

"""##Membangun model CNN"""

from tensorflow.keras.callbacks import (
    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
)
num_classes = 5

model = Sequential([
    InputLayer(input_shape=(30, 128)),

    Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),
    BatchNormalization(),
    Dropout(0.3),

    Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'),
    BatchNormalization(),
    Dropout(0.3),

    Conv1D(filters=16, kernel_size=3, activation='relu', padding='same'),
    BatchNormalization(),
    Dropout(0.3),

    GlobalMaxPooling1D(),

    Dense(64, activation='relu'),
    Dropout(0.4),

    Dense(num_classes, activation='softmax')
])

model.compile(
    optimizer=Adam(learning_rate=0.0005),  # lebih kecil agar stabil
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# === CALLBACKS ===

# Simpan model terbaik
checkpoint_cb = ModelCheckpoint(
    "best_cnn_tf_model.h5",
    save_best_only=True,
    monitor="val_loss",
    mode="min"
)

# Hentikan training jika overfitting mulai muncul
earlystop_cb = EarlyStopping(
    monitor='val_loss',
    patience=4,
    restore_best_weights=True
)

# Turunkan learning rate otomatis jika tidak membaik
reduce_lr_cb = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,      # turunkan LR 50%
    patience=2,      # jika 2 epoch tidak membaik
    min_lr=1e-6
)

# Training
history = model.fit(
    X_train_w2v, y_train,
    validation_data=(X_val_w2v, y_val),
    epochs=100,
    batch_size=32,
    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb]
)

# earlystop_cb = EarlyStopping(
#     monitor='val_loss',
#     patience=10,
#     restore_best_weights=True
# )

# history = model.fit(
#     X_train_w2v, y_train,
#     validation_data=(X_val_w2v, y_val),
#     epochs=100,
#     batch_size=32,
#     callbacks=[checkpoint_cb, earlystop_cb]
# )

# === PLOTTING AKURASI & LOSS ===
# Accuracy
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training vs Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training vs Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

"""#Evaluasi Model

##TF-IDF
"""

vectorizer = TfidfVectorizer(max_features=n)
X_train_tfidf = vectorizer.fit_transform(train_opt_df['statement'])
X_test_tfidf = vectorizer.transform(test_df['statement'])

"""##SVC"""

# svc_model = LinearSVC()
# svc_model.fit(X_train_tfidf, y_train)
# svc_preds = svc_model.predict(X_test_tfidf)

# print("=== LinearSVC ===")
# print(classification_report(y_test, svc_preds, target_names=['Anxiety', 'Depression', 'Normal', 'Personality disorder', 'Suicidal']))
# print("Akurasi:", accuracy_score(y_test, svc_preds))

"""##CatBoost"""

# catboost_model = CatBoostClassifier(verbose=0)
# catboost_model.fit(X_train_tfidf, y_train)
# catboost_preds = (catboost_model.predict(X_test_tfidf) > 0.5).astype(int)

# print("\n=== CatBoost ===")
# print(classification_report(y_test, catboost_preds, target_names=['Anxiety', 'Depression', 'Normal', 'Personality disorder', 'Suicidal']))
# print("Akurasi:", accuracy_score(y_test, catboost_preds))

"""# XGBoost"""

from xgboost import XGBClassifier

num_classes = 5

xgb = XGBClassifier(
    objective='multi:softmax',   # langsung output kelas 0..num_classes-1
    num_class=num_classes,
    eval_metric='mlogloss'
)

xgb.fit(X_train_tfidf, y_train)

xgb_preds = xgb.predict(X_test_tfidf)

print("\n=== XGBoost ===")
print(classification_report(y_test, xgb_preds,
                            target_names=['Anxiety', 'Depression', 'Normal', 'Personality disorder', 'Suicidal']))
print("Akurasi:", accuracy_score(y_test, xgb_preds))

results_df = test_df.copy()
results_df['label_asli'] = y_test
# results_df['pred_svc'] = svc_preds
# results_df['pred_catboost'] = catboost_preds
results_df['pred_xgboost'] = xgb_preds
results_df.to_csv("hasil_prediksi_tfidf.csv", index=False)

hasil=pd.read_csv("hasil_prediksi_tfidf.csv")
hasil

print(results_df.head())

print(results_df.columns)

labels = ['Anxiety', 'Depression', 'Normal', 'Personality disorder', 'Suicidal']
from sklearn.metrics import confusion_matrix

# cm_cat = confusion_matrix(results_df['label_asli'], results_df['pred_catboost'])
# print(cm_cat)
# cm_svc = confusion_matrix(results_df['label_asli'], results_df['pred_svc'])
# print(cm_svc)
cm_xgb = confusion_matrix(results_df['label_asli'], results_df['pred_xgboost'])
print(cm_xgb)
import seaborn as sns
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# sns.heatmap(cm_svc, annot=True, fmt="d", cmap="Blues",
#             xticklabels=labels, yticklabels=labels, ax=axes[0])
# axes[0].set_title("SVC")

# sns.heatmap(cm_cat, annot=True, fmt="d", cmap="Greens",
#             xticklabels=labels, yticklabels=labels, ax=axes[1])
# axes[1].set_title("CatBoost")

sns.heatmap(cm_xgb, annot=True, fmt="d", cmap="Oranges",
            xticklabels=labels, yticklabels=labels, ax=axes[0])
axes[2].set_title("XGBoost")

plt.tight_layout()
plt.show()

import joblib

joblib.dump(xgb, "xgb_model.pkl")
joblib.dump(vectorizer, "tfidf_vectorizer.pkl")

joblib.dump(label_map, "label_map.pkl")